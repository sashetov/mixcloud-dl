#!/bin/env python3
"""
downloads a mix from mixcloud page url
"""
import fcntl
import json
import glob
import math
import multiprocessing
from multiprocessing import Process
import os
from os import path
import shutil
import sys
import requests
WORKDIR_PRE = "./.mc_work"
DASH2_URL_PRE = "https://audio6.mixcloud.com/secure/dash2/"
FRAG_LEN = 10 #fragment duration in seconds
def divide_into_chunks(l, n):
    """
    divides a list into n chunks
    """
    for i in range(0, len(l), n):
        yield l[i:i + n]
def run_in_parallel(*fns):
    """
    runs a list of python functions with 0 args each
    as a seperate process
    """
    proc = []
    for fn in fns:
        p = Process(target=fn)
        p.start()
        proc.append(p)
    for p in proc:
        p.join()
def add_funcs(funcs, func, args_generator, *args_generator_args):
    """
    adds callable lambdas with scope wrapped local vars to call func with
    those callable lambdas are generated by a passed function
    and args are passed to the passed function
    """
    argses = args_generator(*args_generator_args)
    for args in argses:
        cur_func = (lambda v: lambda: func(*v) or \
            print(func, *v, " failed "))(args)
        funcs.append(cur_func)
    return True
def start_workers(funcs, nproc):
    """
    runs funcs in chunks of nproc at a time
    nproc may or may not map to number of cores on your system, test this out..
    """
    func_chunks = list(divide_into_chunks(funcs, nproc)) # chunkify
    i = 1
    for chunk in func_chunks: # process n chunks of size nproc at a time
        print("worker:{}/{} nproc:{}".format(i, len(func_chunks), nproc))
        run_in_parallel(*chunk)
        i += 1
    return True
def make_workdir(username, mix):
    """
    makes a temporary work dir where to download fragment files
    """
    workdir = f"{WORKDIR_PRE}_{username}_{mix}"
    try:
        os.mkdir(workdir)
    except FileExistsError:
        pass
    except Exception as error: # pylint: disable=broad-except
        print(f"ERR: {error}")
        return False
    return workdir
def delete_workdir(workdir_path):
    """
    recursively deletes a dir
    """
    try:
        shutil.rmtree(workdir_path)
    except Exception as error: # pylint: disable=broad-except
        print(f"ERR: {error}")
        return False
    return True
def get_sess_cookies(sess, url):
    """
    gets a url's cookies with get request with session
    """
    sess.get(url)
    return sess.cookies.get_dict()
def mc_graphql_get_data(sess, csrftoken, sess_cookies, url, username, mix):
    """
    queries mixcloud graphql endpoint to get the path string for downloading
    the fragments and also the duration length of the mix
    """
    mc_graphql_url = "https://www.mixcloud.com/graphql"
    req_headers = {
        "Content-Type"  : "application/json",
        "X-CSRFToken"   : csrftoken,
        "Referer"       : url
    }
    query = {
        "query" : """
        query CloudcastHeader($vars:CloudcastLookup!) {
            cloudcastLookup(lookup:$vars) {id,...F2}
        }
        fragment F0 on Cloudcast {id,waveformUrl,audioLength,isPlayable}
        fragment F1 on Cloudcast {id,name,slug,owner {username,id}}
        fragment F2 on Cloudcast {id,owner {id},...F0,...F1}
        """,
        "variables": {
            "vars": {
                "username" : username,
                "slug" : mix
            }
        }
    }
    res = sess.post(mc_graphql_url,
                    headers=req_headers,
                    cookies=sess_cookies,
                    json=query)
    if res.status_code != 200:
        print(f"BAD STATUS: {res.status_code} for url {mc_graphql_url}")
        print(json.dumps(res.json(), indent=1))
        return False
    res = res.json()
    return res
def download_url_content(url):
    """
    downloads binary content from url, streaming it
    returns data or False
    """
    res = requests.get(url, timeout=30, stream=True)
    if res.status_code != 200:
        print(f"BAD STATUS: {res.status_code} for url {url}")
        return False
    content = res.content
    return content
def download_url_to_path(url, dl_path):
    """
    downloads binary content from url, streaming it
    writes content to dl_path
    """
    res = requests.get(url, timeout=30, stream=True)
    if res.status_code != 200:
        print(f"BAD STATUS: {res.status_code} for url {url}")
        return False
    content = res.content
    f = open(dl_path, "wb", 1)
    fcntl.flock(f, fcntl.LOCK_EX)
    f.write(content)
    fcntl.flock(f, fcntl.LOCK_UN)
    f.close()
    return True
def cat(outfile, *from_files):
    """
    binary appends from_files to outfile similar to
    $ cat a b c d > oufile
    <=>
    cat("outfile", "a", "b", "c", "d")
    """
    f = open(outfile, "wb+")
    for from_filename in from_files:
        with open(from_filename, "rb") as from_file:
            f.write(from_file.read())
def generate_dl_mp4_frags_args(gql_res, workdir):
    """
    generates lol (list of lists) of args to be run with download_url_to_path
    function with format:
        [[url0,path0],..[urlN,pathN]]
    and returns this
    """
    args = []
    waveform_url = gql_res["data"]["cloudcastLookup"]["waveformUrl"]
    audio_length = int(gql_res["data"]["cloudcastLookup"]["audioLength"])
    wf_path = waveform_url.split('.json')[0][30:]
    num_frags = math.ceil(audio_length / FRAG_LEN)
    mp4_url_pre = f"{DASH2_URL_PRE}/{wf_path}.m4a"
    init_fname = "init-a1-x3.mp4"
    mp4_url = f"{mp4_url_pre}/{init_fname}"
    init_out = f"{workdir}/0-{init_fname}"
    if not path.exists(init_out):
        args.append([mp4_url, init_out])
    else:
        print(f"init_exists: {init_out}")
    i = 1
    while i < num_frags:
        frag_fname = f"fragment-{i}-a1-x3.m4s"
        mp4_url = f"{mp4_url_pre}/{frag_fname}"
        frag_out = f"{workdir}/{frag_fname}"
        if path.exists(frag_out):
            print(f"fragment_exists: {frag_out} {i}/{num_frags}")
            i += 1
            continue
        args.append([mp4_url, frag_out])
        i += 1
    return args
def mc_dl_mp4(gql_res, username, mix, nproc):
    """
    makes a workdir and starts downloading the waveform parts every 2 seconds
    joins them together into an mp4 file after each is downloaded
    cleans up after itself when done or on exception
    """
    mkdir_status = make_workdir(username, mix)
    if not mkdir_status:
        return 1
    workdir = mkdir_status
    mp4_filename = f"./{username}--{mix}.mp4"
    funcs = []
    args = generate_dl_mp4_frags_args(gql_res, workdir)
    print(args)
    add_funcs(
        funcs, download_url_to_path, generate_dl_mp4_frags_args,
        gql_res, workdir)
    try:
        start_workers(funcs, nproc)
    except KeyboardInterrupt:
        pass
    mp4_frags = list(glob.glob(f"{workdir}"+"/*"))
    mp4_frags.sort()
    cat(mp4_filename, *mp4_frags)
    delete_status = delete_workdir(workdir)
    if not delete_status:
        return 2
    return 0

def main():
    """
    entry point for program
    processes args ( URL )
    """
    prog_name = sys.argv[0]
    args = sys.argv[1:]
    argc = len(args)
    if argc < 1:
        print(f"USAGE: {prog_name} [MIXCLOUD_URL] (NUM_PROC)\n\n"+
              "NOTE: args in [] are required\n      args in () are not")
        sys.exit(1)
    url = args[0]
    if argc > 1:
        nproc = int(args[1])
    else:
        nproc = multiprocessing.cpu_count()
    url_parts = url.split("/")
    username = url_parts[3]
    mix = url_parts[4]
    url = "/".join(url_parts[0:5]) + "/"# get rid of anything after mix name
    sess = requests.Session()
    sess_cookies = get_sess_cookies(sess, url)
    csrftoken = sess_cookies["csrftoken"]
    gql_res = mc_graphql_get_data(
        sess, csrftoken, sess_cookies, url, username, mix)
    print(json.dumps(gql_res, indent=1))
    if not gql_res:
        sys.exit(1)
    sys.exit(mc_dl_mp4(gql_res, username, mix, nproc))
if __name__ == "__main__":
    main()
